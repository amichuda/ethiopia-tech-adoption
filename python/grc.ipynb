{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "from numba import jit"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "df = pd.read_stata(\"../ethiopia_data/data/processed/full_panel.dta\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/lib/python3.9/site-packages/pandas/io/stata.py:1457: UnicodeWarning: \n",
      "One or more strings in the dta file could not be decoded using utf-8, and\n",
      "so the fallback encoding of latin-1 is being used.  This can happen when a file\n",
      "has been incorrectly encoded by Stata or some other software. You should verify\n",
      "the string values returned are correct.\n",
      "  warnings.warn(msg, UnicodeWarning)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "df['crop_code'].dtype"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "CategoricalDtype(categories=[          'BARLEY',            'MAIZE',           'MILLET',\n",
       "                              'OATS',             'RICE',          'SORGHUM',\n",
       "                              'TEFF',            'WHEAT',            'ROMAN',\n",
       "                           'CASSAVA',\n",
       "                  ...\n",
       "                       'TIMIZ KIMEM',     'OTHER SPICES',     'OTHER PULSES',\n",
       "                    'OTHER OIL SEED',     'OTHER CEREAL', 'OTHER CASH CROPS',\n",
       "                            'OTHERS',  'OTHER VEGETABLE',                124,\n",
       "                                 126],\n",
       ", ordered=True)"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "trajectories = (\n",
    "    df\n",
    "    .query(\"crop_code == 'MAIZE'\")\n",
    "    .dropna(subset= ['impmaize'])\n",
    "    .groupby(['holder_id', 'parcel_id', 'field_id'])['impmaize']\n",
    "    .agg(trajectories = list)\n",
    "    .assign(len_traj = lambda df: df['trajectories'].apply(lambda x: len(x)))\n",
    "    .query(\"len_traj == 3\")\n",
    "    .drop(['len_traj'], axis=1)\n",
    "    .assign(trajectories = lambda df: df['trajectories'].astype(str))\n",
    "    .pipe(pd.get_dummies)\n",
    "    .rename(lambda x: x.replace('.0', '').replace(',', '').replace('[', '').replace(']', '').replace(' ', ''), axis=1)\n",
    "    )\n",
    "\n",
    "# merge with df\n",
    "\n",
    "merged_df = (\n",
    "    df\n",
    "    .query(\"crop_code == 'MAIZE'\")\n",
    "    .merge(trajectories, \n",
    "           left_on= ['holder_id', 'parcel_id', 'field_id'], \n",
    "           right_index=True)\n",
    "\n",
    "    )\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "merged_df.columns[merged_df.columns.str.contains(\"trajectories_\")][1:7]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index(['trajectories_001', 'trajectories_010', 'trajectories_011',\n",
       "       'trajectories_100', 'trajectories_101', 'trajectories_110'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "reg_dict = {}\n",
    "\n",
    "outcomes = merged_df.columns[merged_df.columns.str.contains('YIELD')].tolist()\n",
    "h_switchers = merged_df.columns[merged_df.columns.str.contains(\"trajectories_\")][1:7].tolist()\n",
    "h_switchers_int = [f\"{i}:impmaize\" for i in h_switchers]\n",
    "h_no_always = merged_df.columns[merged_df.columns.str.contains(\"trajectories_\")][0:7].tolist()\n",
    "\n",
    "merged_df_dropna = merged_df.dropna(subset= outcomes + ['impmaize'] + h_no_always + h_switchers)\n",
    "\n",
    "for y in outcomes:\n",
    "\n",
    "    reg_dict[y] = smf.ols(f\"np.arcsinh({y}) ~ -1 + {' + '.join(h_no_always)} + {' + '.join(h_switchers_int)}\", \n",
    "                          data = merged_df_dropna)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# Now run weak-id test\n",
    "reg_res_dict = {}\n",
    "\n",
    "for y, mod in reg_dict.items():\n",
    "    print(f\"Trying {y}\")\n",
    "    res = mod.fit(cov_type = 'cluster', cov_kwds = {'groups' : merged_df_dropna['holder_id']})\n",
    "    reg_res_dict[y] = res\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Trying YIELD_cropcutfresh\n",
      "Trying YIELD_cropcutdry\n",
      "Trying YIELD_cropcutfresh_tr\n",
      "Trying YIELD_cropcutdry_tr\n",
      "Trying YIELD_selfr\n",
      "Trying YIELD_selfr_tr\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "np.arange(-50000, 50000, 0.1).size"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "source": [
    "def weak_id_test(res, start=-100, stop=100, inc=0.1):\n",
    "    trajectories = np.array([\"010\", \"011\", \"100\", \"101\", \"110\"])\n",
    "    ranger = np.arange(start, stop, inc)\n",
    "    mat = np.zeros((ranger.size, trajectories.size))\n",
    "    for i, phi in enumerate(tqdm(ranger)):\n",
    "        for j, traj in enumerate(trajectories):\n",
    "            test = f\"trajectories_{traj} - trajectories_001 = {phi}*(trajectories_{traj}:impmaize - trajectories_001:impmaize)\"\n",
    "            mat[i, j] = res.t_test(test).pvalue\n",
    "            \n",
    "    df= pd.DataFrame(columns = trajectories,\n",
    "                 index = pd.Index(ranger),\n",
    "                 data=mat)\n",
    "            \n",
    "    return df\n",
    "\n",
    "def weak_id_joint_test(res, start=-100, stop=100, inc=0.1):\n",
    "    \n",
    "    trajectories = np.array([\"010\", \"011\", \"100\", \"101\", \"110\"])\n",
    "    ranger = np.arange(start, stop, inc)\n",
    "    mat = np.zeros(ranger.size)\n",
    "    \n",
    "    for i, phi in enumerate(tqdm(ranger)):\n",
    "        joint_test_list = [f\"(trajectories_{traj} - trajectories_001 = {phi}*(trajectories_{traj}:impmaize - trajectories_001:impmaize))\" \\\n",
    "            for traj in trajectories]\n",
    "    \n",
    "        joint_test = ' , '.join(joint_test_list)\n",
    "        mat[i] = res.f_test(joint_test).pvalue\n",
    "\n",
    "    df= pd.DataFrame(columns = ['joint'],\n",
    "                 index = pd.Index(ranger),\n",
    "                 data=mat)\n",
    "            \n",
    "    return df\n",
    "\n",
    "def phi_ci(weak_id_df):\n",
    "    \n",
    "    phi_p_min = weak_id_df[weak_id_df.apply(lambda x: x > 0.05)].min()\n",
    "    phi_p_max = weak_id_df[weak_id_df.apply(lambda x: x > 0.05)].max()\n",
    "    \n",
    "    phi_df = pd.DataFrame(\n",
    "        index = ['min', 'max'],\n",
    "        columns = weak_id_df.columns\n",
    "    )\n",
    "    \n",
    "    for col, mi, ma in zip(weak_id_df.columns, phi_p_min, phi_p_max):\n",
    "        try:\n",
    "            phi_df.loc['min', col] = weak_id_df.index[weak_id_df[col] == mi].values[0]\n",
    "            phi_df.loc['max', col] = weak_id_df.index[weak_id_df[col] == ma].values[0]\n",
    "        except IndexError:\n",
    "            print(f\"\"\"Might be NaNs: \n",
    "                  phi_min = {phi_p_min.values[0]}\n",
    "                  phi_max = {phi_p_max.values[0]}\n",
    "                  \"\"\")\n",
    "        \n",
    "    return phi_df\n",
    "        "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "source": [
    "weak_id = weak_id_test(reg_res_dict['YIELD_cropcutfresh'])"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "89cce4187ea949e1a2786f2798e7fc33"
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "source": [
    "weak_id_joint = weak_id_joint_test(reg_res_dict['YIELD_cropcutfresh'])"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "29bd411545434a32aa7057e33de84872"
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/lib/python3.9/site-packages/statsmodels/base/model.py:1832: ValueWarning: covariance of constraints does not have full rank. The number of constraints is 5, but rank is 4\n",
      "  warnings.warn('covariance of constraints does not have full '\n",
      "/usr/lib/python3.9/site-packages/statsmodels/base/model.py:1832: ValueWarning: covariance of constraints does not have full rank. The number of constraints is 5, but rank is 4\n",
      "  warnings.warn('covariance of constraints does not have full '\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "source": [
    "phi_ci(weak_id_joint)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Might be NaNs: \n",
      "                  phi_min = nan\n",
      "                  phi_max = nan\n",
      "                  \n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>joint</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    joint\n",
       "min   NaN\n",
       "max   NaN"
      ]
     },
     "metadata": {},
     "execution_count": 99
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "source": [
    "weak_id_joint[weak_id_joint.apply(lambda x: x > 0.05)]."
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "joint   NaN\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "execution_count": 95
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit"
  },
  "interpreter": {
   "hash": "4e1d9a8909477db77738c33245c29c7265277ef753467dede8cf3f814cde494e"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}